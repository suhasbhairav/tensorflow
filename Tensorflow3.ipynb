{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmiwhuW4axlYZcWwwdhV4Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "810zab9Qk3-p",
        "outputId": "aeaf9270-2b60-4e58-c942-16bbbaa2ec4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 36s 45ms/step - loss: 0.4322 - accuracy: 0.8012 - val_loss: 0.3027 - val_accuracy: 0.8768\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.2041 - accuracy: 0.9248 - val_loss: 0.2869 - val_accuracy: 0.8855\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Prepare the dataset\n",
        "vocab_size = 40000  # Only consider the top 20k words\n",
        "maxlen = 400  # Only consider the first 200 words of each sequence\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "\n",
        "# Build the model\n",
        "embed_dim = 64  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 64  # Hidden layer size in feed forward network inside the transformer\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
        "x = layers.GlobalAveragePooling1D()(embedding_layer)\n",
        "x = layers.Dense(50, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
      ]
    }
  ]
}